{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VW_tLtF1g-1L"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define the grid size\n",
        "grid_size = 4\n",
        "\n",
        "# Define the rewards for each state\n",
        "rewards = np.full((grid_size, grid_size), -1.0)  # Initialize with -1 for all moves\n",
        "rewards[grid_size - 1, grid_size - 1] = 0  # Goal state reward\n",
        "\n",
        "# Define value function (initialize to zero)\n",
        "values = np.zeros((grid_size, grid_size))\n",
        "\n",
        "# Define parameters\n",
        "gamma = 0.9  # Discount factor\n",
        "theta = 1e-4  # Convergence threshold\n",
        "max_policy_eval_iterations = 10  # Max iterations for policy evaluation\n",
        "actions = ['up', 'down', 'left', 'right']\n",
        "action_deltas = {'up': (-1, 0), 'down': (1, 0), 'left': (0, -1), 'right': (0, 1)}\n",
        "\n",
        "# Function to check if a state is terminal\n",
        "def is_terminal_state(x, y):\n",
        "    return (x == grid_size - 1 and y == grid_size - 1)\n",
        "\n",
        "# Function to get the next state given an action\n",
        "def get_next_state(x, y, action):\n",
        "    if action in action_deltas:\n",
        "        delta = action_deltas[action]\n",
        "        new_x, new_y = x + delta[0], y + delta[1]\n",
        "        if 0 <= new_x < grid_size and 0 <= new_y < grid_size:\n",
        "            return new_x, new_y\n",
        "    return x, y  # No change if out of bounds or invalid action\n",
        "\n",
        "# Initialize a random policy (for simplicity, let's start with 'right' for all non-terminal states)\n",
        "policy = np.full((grid_size, grid_size), 'right', dtype=str)\n",
        "policy[grid_size - 1, grid_size - 1] = 'Goal'  # Terminal state policy\n",
        "\n",
        "# Policy Iteration Algorithm with truncated policy evaluation\n",
        "is_policy_stable = False\n",
        "iteration = 0\n",
        "\n",
        "while not is_policy_stable:\n",
        "    # Policy Evaluation\n",
        "    for eval_iteration in range(max_policy_eval_iterations):\n",
        "        delta = 0\n",
        "        new_values = np.copy(values)\n",
        "        for x in range(grid_size):\n",
        "            for y in range(grid_size):\n",
        "                if is_terminal_state(x, y):\n",
        "                    continue\n",
        "                action = policy[x, y]\n",
        "                new_x, new_y = get_next_state(x, y, action)\n",
        "                reward = rewards[new_x, new_y]\n",
        "                new_values[x, y] = reward + gamma * values[new_x, new_y]\n",
        "                delta = max(delta, abs(new_values[x, y] - values[x, y]))\n",
        "        values = new_values\n",
        "        if delta < theta:\n",
        "            break\n",
        "\n",
        "    # Policy Improvement\n",
        "    is_policy_stable = True\n",
        "    for x in range(grid_size):\n",
        "        for y in range(grid_size):\n",
        "            if is_terminal_state(x, y):\n",
        "                continue\n",
        "            old_action = policy[x, y]\n",
        "            best_action = None\n",
        "            best_value = float('-inf')\n",
        "            for action in actions:\n",
        "                new_x, new_y = get_next_state(x, y, action)\n",
        "                reward = rewards[new_x, new_y]\n",
        "                value = reward + gamma * values[new_x, new_y]\n",
        "                if value > best_value:\n",
        "                    best_value = value\n",
        "                    best_action = action\n",
        "            policy[x, y] = best_action\n",
        "            if old_action != best_action:\n",
        "                is_policy_stable = False\n",
        "\n",
        "    iteration += 1\n",
        "    if is_policy_stable:\n",
        "        break\n",
        "\n",
        "# Display the final value function\n",
        "print(\"Value Function after Convergence:\")\n",
        "print(values)\n",
        "print(f\"Converged in {iteration} iterations.\")\n",
        "\n",
        "# Display the optimal policy\n",
        "print(\"\\nOptimal Policy:\")\n",
        "for row in policy:\n",
        "    print(row)\n"
      ]
    }
  ]
}